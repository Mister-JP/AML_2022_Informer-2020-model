{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Informer on S&P 500 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom data (xxx.csv) has to include at least 2 features: date(format: YYYY-MM-DD hh:mm:ss) and target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaIHYx4_ECL"
   },
   "source": [
    "## Set Path and install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'SNP500' in sys.path:\n",
    "    sys.path += ['SNP500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YW9TS6jp_YXc"
   },
   "outputs": [],
   "source": [
    "!pip install -r ./Informer2020/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RPdt-Kwc_RRZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amartyadutta/env/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe",
    "tags": []
   },
   "source": [
    "# Experiments: Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6mx2dnwY9dWi"
   },
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = 'data' # root path of data file\n",
    "args.data_path = 'sp500_close_nrm.csv' # data file\n",
    "args.features = 'S' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'Close' # target feature in S or MS task\n",
    "args.freq = 'd' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = 'informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 32 # input sequence length of Informer encoder\n",
    "args.label_len = 8 # start token length of Informer decoder\n",
    "args.pred_len = 2 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 5 # encoder input size\n",
    "args.dec_in = 5 # decoder input size\n",
    "args.c_out = 5 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 10 # num of heads\n",
    "args.e_layers = 4 # num of encoder layers\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'd'\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 20\n",
    "args.patience = 15\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = True\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k_BCYODAwKl9"
   },
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "53o3pZ809p-a"
   },
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'custom':{'data':'sp500_close_nrm.csv','T':'Close','M':[6,6,6],'S':[1,1,1],'MS':[6,6,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yZ5Q2vyKwSfk"
   },
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywY-umrw-mHO",
    "outputId": "2618b6c1-eb39-430b-e1f6-64e621f7a34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom', 'root_path': 'data', 'data_path': 'sp500_close_nrm.csv', 'features': 'S', 'target': 'Close', 'freq': 'd', 'checkpoints': 'informer_checkpoints', 'seq_len': 32, 'label_len': 8, 'pred_len': 2, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 10, 'e_layers': 4, 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 20, 'patience': 15, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': True, 'devices': '0,1,2,3', 'device_ids': [0, 1, 2, 3], 'detail_freq': 'd'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KVHZhRB4-on9"
   },
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'sp500_close_nrm_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "928tzaA2AA2g",
    "outputId": "4c12a6f6-3c31-401a-ef6a-b034da388593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : sp500_close_nrm_informer_custom_ftS_sl32_ll8_pl2_dm512_nh10_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "data.data_loader.Dataset_Custom\n",
      "train\n",
      "data_path ->  sp500_close_nrm.csv\n",
      "scaling\n",
      "data.data_loader.Dataset_Custom\n",
      "val\n",
      "data_path ->  sp500_close_nrm.csv\n",
      "scaling\n",
      "data.data_loader.Dataset_Custom\n",
      "test\n",
      "data_path ->  sp500_close_nrm.csv\n",
      "scaling\n",
      "loss ->  tensor(0.7330, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(2.1224, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.8331, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.7487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.7461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2582, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3856, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3786, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2237, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3899, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3362, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1955, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1838, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2371, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1809, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1304, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2289, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2639, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1744, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1888, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1943, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1767, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1207, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1638, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1359, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1002, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1277, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1720, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1639, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1669, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1394, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1652, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1359, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1169, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1638, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1416, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1106, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1351, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1409, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1133, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1057, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1131, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1201, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0925, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0828, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1016, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0887, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1305, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1163, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0972, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0882, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1018, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1265, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1160, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1695, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0859, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1790, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1839, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0824, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 1 cost time: 16.69317126274109\n",
      "Epoch: 1, Steps: 68 | Train Loss: 0.2261099 Vali Loss: 0.0763601 Test Loss: 0.3137976\n",
      "Validation loss decreased (inf --> 0.076360).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "loss ->  tensor(0.1849, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1781, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0868, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2078, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0820, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1393, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1222, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1175, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1214, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0859, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1059, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1196, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0770, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1067, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1017, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0829, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1014, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0914, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0786, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1523, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0905, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0878, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0846, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0659, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1368, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1244, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0910, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1166, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0806, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0967, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0847, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1085, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1015, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0928, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0996, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0858, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0669, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0719, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0696, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0724, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0768, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0665, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0636, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0691, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0760, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0710, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0682, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0805, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0771, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0633, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0663, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0587, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0644, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0715, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0683, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0833, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0624, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0982, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0595, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0893, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1044, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0887, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0781, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0872, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0682, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 2 cost time: 7.736799001693726\n",
      "Epoch: 2, Steps: 68 | Train Loss: 0.0939264 Vali Loss: 0.0847749 Test Loss: 0.2443394\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 5e-05\n",
      "loss ->  tensor(0.0717, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0729, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0650, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0621, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0744, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0682, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0557, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0638, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0723, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0739, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0788, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0737, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0688, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0634, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0679, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0577, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0767, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0702, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0599, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0667, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0590, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0592, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0701, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0565, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0601, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0599, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0638, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0787, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0580, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0662, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0695, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0630, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0548, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0565, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0589, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0737, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0683, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0587, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0593, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0596, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0799, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0622, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0825, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1026, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0723, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0772, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0755, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0662, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0686, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0718, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0602, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0706, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0616, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0659, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0676, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0672, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0588, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 3 cost time: 7.956243276596069\n",
      "Epoch: 3, Steps: 68 | Train Loss: 0.0656345 Vali Loss: 0.0998210 Test Loss: 0.3043627\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Updating learning rate to 2.5e-05\n",
      "loss ->  tensor(0.0670, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0584, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0723, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0589, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0637, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0605, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0568, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0524, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0587, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0645, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0708, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0589, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0562, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0575, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0564, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0633, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0561, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0584, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0578, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0630, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0596, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0624, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0713, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0615, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0641, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0634, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0557, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0658, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0575, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0717, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0820, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0643, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0573, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0604, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0780, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0629, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0587, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0534, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0586, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0601, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 4 cost time: 7.917691707611084\n",
      "Epoch: 4, Steps: 68 | Train Loss: 0.0576143 Vali Loss: 0.0802717 Test Loss: 0.2743624\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Updating learning rate to 1.25e-05\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0556, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0632, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0535, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0419, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0561, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0473, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0439, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0590, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0624, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0478, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0624, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0535, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0630, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0564, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0556, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0573, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0630, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0615, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0633, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 5 cost time: 7.791038274765015\n",
      "Epoch: 5, Steps: 68 | Train Loss: 0.0525471 Vali Loss: 0.0776120 Test Loss: 0.2691627\n",
      "EarlyStopping counter: 4 out of 15\n",
      "Updating learning rate to 6.25e-06\n",
      "loss ->  tensor(0.0425, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0561, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0589, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0415, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0483, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0601, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0667, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0580, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0426, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0582, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0379, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0558, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0557, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0545, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0588, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0434, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0548, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0594, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0607, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0516, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0563, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0564, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0454, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0560, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0588, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0563, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0473, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 6 cost time: 7.686707973480225\n",
      "Epoch: 6, Steps: 68 | Train Loss: 0.0513323 Vali Loss: 0.0921969 Test Loss: 0.2888687\n",
      "EarlyStopping counter: 5 out of 15\n",
      "Updating learning rate to 3.125e-06\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0560, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0569, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0566, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0565, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0523, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0546, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0555, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0593, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0408, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0400, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0524, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0401, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0588, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0406, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0628, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0361, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0592, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0389, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0516, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0562, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0574, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0617, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 7 cost time: 7.655312538146973\n",
      "Epoch: 7, Steps: 68 | Train Loss: 0.0507953 Vali Loss: 0.0706159 Test Loss: 0.2505274\n",
      "Validation loss decreased (0.076360 --> 0.070616).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0447, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0566, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0516, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0546, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0418, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0433, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0409, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0577, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0434, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0629, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0609, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0595, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0559, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0416, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0450, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0445, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0421, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0545, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0407, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0450, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0585, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 8 cost time: 7.707916498184204\n",
      "Epoch: 8, Steps: 68 | Train Loss: 0.0496596 Vali Loss: 0.0632637 Test Loss: 0.2374627\n",
      "Validation loss decreased (0.070616 --> 0.063264).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0555, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0413, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0428, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0462, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0478, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0454, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0521, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0605, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0558, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0597, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0566, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0582, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0483, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0414, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0466, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0577, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0464, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0418, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0418, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 9 cost time: 7.88810396194458\n",
      "Epoch: 9, Steps: 68 | Train Loss: 0.0497974 Vali Loss: 0.0625324 Test Loss: 0.2357908\n",
      "Validation loss decreased (0.063264 --> 0.062532).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0440, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0555, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0422, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0413, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0545, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0433, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0431, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0360, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0562, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0435, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0445, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0407, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0439, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 10 cost time: 7.818820238113403\n",
      "Epoch: 10, Steps: 68 | Train Loss: 0.0484992 Vali Loss: 0.0622863 Test Loss: 0.2361623\n",
      "Validation loss decreased (0.062532 --> 0.062286).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0450, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0408, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0541, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0408, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0600, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0478, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0382, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0535, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0434, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0595, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0602, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0584, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0414, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0412, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0437, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0597, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0565, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0454, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0592, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0447, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0412, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0531, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0466, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0433, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 11 cost time: 7.957280874252319\n",
      "Epoch: 11, Steps: 68 | Train Loss: 0.0488395 Vali Loss: 0.0626725 Test Loss: 0.2359917\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 9.765625e-08\n",
      "loss ->  tensor(0.0558, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0573, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0439, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0593, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0571, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0430, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0429, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0466, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0427, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0554, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0548, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0584, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0559, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0367, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0439, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0574, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0383, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0581, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0556, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0473, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0422, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0427, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 12 cost time: 8.008364915847778\n",
      "Epoch: 12, Steps: 68 | Train Loss: 0.0494455 Vali Loss: 0.0606484 Test Loss: 0.2297889\n",
      "Validation loss decreased (0.062286 --> 0.060648).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0466, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0483, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0592, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0379, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0374, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0621, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0531, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0599, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0417, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0400, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0524, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0553, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0655, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0431, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0656, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0572, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0579, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0569, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0425, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0572, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0417, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0582, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0429, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0484, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0356, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 13 cost time: 7.8810179233551025\n",
      "Epoch: 13, Steps: 68 | Train Loss: 0.0502442 Vali Loss: 0.0618298 Test Loss: 0.2317297\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 2.44140625e-08\n",
      "loss ->  tensor(0.0426, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0413, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0408, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0411, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0412, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0422, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0521, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0393, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0523, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0473, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0462, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0413, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0420, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0419, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0568, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0401, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0593, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0630, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0407, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0402, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0542, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0580, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0559, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0464, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0412, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 14 cost time: 7.913286209106445\n",
      "Epoch: 14, Steps: 68 | Train Loss: 0.0485717 Vali Loss: 0.0599224 Test Loss: 0.2290448\n",
      "Validation loss decreased (0.060648 --> 0.059922).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-08\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0430, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0567, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0556, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0450, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0534, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0420, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0435, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0521, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0440, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0388, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0524, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0424, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0387, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0418, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0581, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0517, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0531, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0719, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0450, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0591, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0434, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0411, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0466, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0534, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0566, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0558, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0591, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 15 cost time: 7.925333738327026\n",
      "Epoch: 15, Steps: 68 | Train Loss: 0.0493174 Vali Loss: 0.0602676 Test Loss: 0.2302344\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 6.103515625e-09\n",
      "loss ->  tensor(0.0575, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0431, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0426, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0601, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0396, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0462, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0559, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0415, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0548, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0562, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0544, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0441, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0436, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0516, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0523, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0561, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0531, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0447, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0418, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0428, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0462, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0417, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 16 cost time: 7.808504343032837\n",
      "Epoch: 16, Steps: 68 | Train Loss: 0.0494948 Vali Loss: 0.0619909 Test Loss: 0.2353795\n",
      "EarlyStopping counter: 2 out of 15\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0448, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0458, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0622, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0534, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0435, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0483, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0549, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0428, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0414, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0563, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0520, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0423, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0415, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0587, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0406, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0614, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0512, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0468, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0419, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0377, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0591, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0503, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0559, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0483, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0495, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0502, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0378, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0384, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0427, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0581, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0467, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0436, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0429, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 17 cost time: 7.875483512878418\n",
      "Epoch: 17, Steps: 68 | Train Loss: 0.0486972 Vali Loss: 0.0641529 Test Loss: 0.2403476\n",
      "EarlyStopping counter: 3 out of 15\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0515, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0599, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0510, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0546, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0404, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0456, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0400, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0556, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0416, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0427, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0428, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0469, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0563, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0449, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0536, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0568, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0480, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0511, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0445, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0369, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0396, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0423, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0460, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0425, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0445, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0597, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0452, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0522, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0564, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0445, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0596, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0507, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0454, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0453, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0447, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0527, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0481, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0454, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 18 cost time: 7.700109958648682\n",
      "Epoch: 18, Steps: 68 | Train Loss: 0.0485027 Vali Loss: 0.0593853 Test Loss: 0.2292155\n",
      "Validation loss decreased (0.059922 --> 0.059385).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "loss ->  tensor(0.0451, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0548, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0387, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0546, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0575, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0447, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0422, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0500, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0509, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0532, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0560, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0633, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0530, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0547, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0528, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0431, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0461, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0497, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0519, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0471, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0551, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0427, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0535, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0446, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0455, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0594, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0531, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0576, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0558, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0496, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0523, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0514, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0474, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0529, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0498, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0555, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0416, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0412, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0560, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0524, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0560, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0414, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0459, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 19 cost time: 7.832392454147339\n",
      "Epoch: 19, Steps: 68 | Train Loss: 0.0503772 Vali Loss: 0.0613458 Test Loss: 0.2338845\n",
      "EarlyStopping counter: 1 out of 15\n",
      "Updating learning rate to 3.814697265625e-10\n",
      "loss ->  tensor(0.0472, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0508, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0492, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0552, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0464, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0457, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0476, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0526, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0409, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0446, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0436, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0665, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0416, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0491, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0540, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0489, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0550, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0535, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0513, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0565, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0506, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0493, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0516, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0443, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0553, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0533, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0441, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0462, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0537, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0538, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0543, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0490, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0488, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0409, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0518, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0504, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0542, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0477, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0482, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0470, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0487, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0404, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0610, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0486, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0475, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0464, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0395, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0425, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0501, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0597, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0570, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0494, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0436, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0432, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0539, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0409, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 20 cost time: 7.992471218109131\n",
      "Epoch: 20, Steps: 68 | Train Loss: 0.0490769 Vali Loss: 0.0588537 Test Loss: 0.2327195\n",
      "Validation loss decreased (0.059385 --> 0.058854).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-10\n",
      ">>>>>>>testing : sp500_close_nrm_informer_custom_ftS_sl32_ll8_pl2_dm512_nh10_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "data.data_loader.Dataset_Custom\n",
      "test\n",
      "data_path ->  sp500_close_nrm.csv\n",
      "scaling\n",
      "[[[[1.5952184 ]\n",
      "   [1.57647   ]]\n",
      "\n",
      "  [[1.57647   ]\n",
      "   [1.5362905 ]]\n",
      "\n",
      "  [[1.5362905 ]\n",
      "   [1.5056558 ]]\n",
      "\n",
      "  [[1.5056558 ]\n",
      "   [1.5346454 ]]\n",
      "\n",
      "  [[1.5346454 ]\n",
      "   [1.5415668 ]]\n",
      "\n",
      "  [[1.5415668 ]\n",
      "   [1.5837767 ]]\n",
      "\n",
      "  [[1.5837767 ]\n",
      "   [1.5690299 ]]\n",
      "\n",
      "  [[1.5690299 ]\n",
      "   [1.5639316 ]]\n",
      "\n",
      "  [[1.5639316 ]\n",
      "   [1.5774333 ]]\n",
      "\n",
      "  [[1.5774333 ]\n",
      "   [1.6173608 ]]\n",
      "\n",
      "  [[1.6173608 ]\n",
      "   [1.6177018 ]]\n",
      "\n",
      "  [[1.6177018 ]\n",
      "   [1.6512711 ]]\n",
      "\n",
      "  [[1.6512711 ]\n",
      "   [1.652857  ]]\n",
      "\n",
      "  [[1.652857  ]\n",
      "   [1.6508117 ]]\n",
      "\n",
      "  [[1.6508117 ]\n",
      "   [1.6719017 ]]\n",
      "\n",
      "  [[1.6719017 ]\n",
      "   [1.6953928 ]]\n",
      "\n",
      "  [[1.6953928 ]\n",
      "   [1.6995279 ]]\n",
      "\n",
      "  [[1.6995279 ]\n",
      "   [1.6985942 ]]\n",
      "\n",
      "  [[1.6985942 ]\n",
      "   [1.7230932 ]]\n",
      "\n",
      "  [[1.7230932 ]\n",
      "   [1.7232561 ]]\n",
      "\n",
      "  [[1.7232561 ]\n",
      "   [1.6954966 ]]\n",
      "\n",
      "  [[1.6954966 ]\n",
      "   [1.7095616 ]]\n",
      "\n",
      "  [[1.7095616 ]\n",
      "   [1.7496818 ]]\n",
      "\n",
      "  [[1.7496818 ]\n",
      "   [1.7155937 ]]\n",
      "\n",
      "  [[1.7155937 ]\n",
      "   [1.732534  ]]\n",
      "\n",
      "  [[1.732534  ]\n",
      "   [1.719047  ]]\n",
      "\n",
      "  [[1.719047  ]\n",
      "   [1.7425678 ]]\n",
      "\n",
      "  [[1.7425678 ]\n",
      "   [1.774655  ]]\n",
      "\n",
      "  [[1.774655  ]\n",
      "   [1.7607975 ]]\n",
      "\n",
      "  [[1.7607975 ]\n",
      "   [1.7945595 ]]\n",
      "\n",
      "  [[1.7945595 ]\n",
      "   [1.7871786 ]]\n",
      "\n",
      "  [[1.7871786 ]\n",
      "   [1.7962787 ]]]\n",
      "\n",
      "\n",
      " [[[1.7962787 ]\n",
      "   [1.8370658 ]]\n",
      "\n",
      "  [[1.8370658 ]\n",
      "   [1.8560513 ]]\n",
      "\n",
      "  [[1.8560513 ]\n",
      "   [1.8429645 ]]\n",
      "\n",
      "  [[1.8429645 ]\n",
      "   [1.8443873 ]]\n",
      "\n",
      "  [[1.8443873 ]\n",
      "   [1.8500044 ]]\n",
      "\n",
      "  [[1.8500044 ]\n",
      "   [1.805438  ]]\n",
      "\n",
      "  [[1.805438  ]\n",
      "   [1.7286065 ]]\n",
      "\n",
      "  [[1.7286065 ]\n",
      "   [1.7769374 ]]\n",
      "\n",
      "  [[1.7769374 ]\n",
      "   [1.7727283 ]]\n",
      "\n",
      "  [[1.7727283 ]\n",
      "   [1.7879345 ]]\n",
      "\n",
      "  [[1.7879345 ]\n",
      "   [1.7017659 ]]\n",
      "\n",
      "  [[1.7017659 ]\n",
      "   [1.7364467 ]]\n",
      "\n",
      "  [[1.7364467 ]\n",
      "   [1.80858   ]]\n",
      "\n",
      "  [[1.80858   ]\n",
      "   [1.8635656 ]]\n",
      "\n",
      "  [[1.8635656 ]\n",
      "   [1.8800019 ]]\n",
      "\n",
      "  [[1.8800019 ]\n",
      "   [1.8532206 ]]\n",
      "\n",
      "  [[1.8532206 ]\n",
      "   [1.8893539 ]]\n",
      "\n",
      "  [[1.8893539 ]\n",
      "   [1.8977425 ]]\n",
      "\n",
      "  [[1.8977425 ]\n",
      "   [1.9299039 ]]\n",
      "\n",
      "  [[1.9299039 ]\n",
      "   [1.9217376 ]]\n",
      "\n",
      "  [[1.9217376 ]\n",
      "   [1.9309561 ]]\n",
      "\n",
      "  [[1.9309561 ]\n",
      "   [1.916328  ]]\n",
      "\n",
      "  [[1.916328  ]\n",
      "   [1.9398339 ]]\n",
      "\n",
      "  [[1.9398339 ]\n",
      "   [1.9206853 ]]\n",
      "\n",
      "  [[1.9206853 ]\n",
      "   [1.8681008 ]]\n",
      "\n",
      "  [[1.8681008 ]\n",
      "   [1.7023143 ]]\n",
      "\n",
      "  [[1.7023143 ]\n",
      "   [1.5575438 ]]\n",
      "\n",
      "  [[1.5575438 ]\n",
      "   [1.5400255 ]]\n",
      "\n",
      "  [[1.5400255 ]\n",
      "   [1.3360455 ]]\n",
      "\n",
      "  [[1.3360455 ]\n",
      "   [1.299675  ]]\n",
      "\n",
      "  [[1.299675  ]\n",
      "   [1.501254  ]]\n",
      "\n",
      "  [[1.501254  ]\n",
      "   [1.3725197 ]]]\n",
      "\n",
      "\n",
      " [[[1.3725197 ]\n",
      "   [1.5603745 ]]\n",
      "\n",
      "  [[1.5603745 ]\n",
      "   [1.4030063 ]]\n",
      "\n",
      "  [[1.4030063 ]\n",
      "   [1.3265749 ]]\n",
      "\n",
      "  [[1.3265749 ]\n",
      "   [0.9919043 ]]\n",
      "\n",
      "  [[0.9919043 ]\n",
      "   [1.1929795 ]]\n",
      "\n",
      "  [[1.1929795 ]\n",
      "   [0.9842271 ]]\n",
      "\n",
      "  [[0.9842271 ]\n",
      "   [0.59778714]]\n",
      "\n",
      "  [[0.59778714]\n",
      "   [0.93923086]]\n",
      "\n",
      "  [[0.93923086]\n",
      "   [0.4577149 ]]\n",
      "\n",
      "  [[0.4577149 ]\n",
      "   [0.6697426 ]]\n",
      "\n",
      "  [[0.6697426 ]\n",
      "   [0.4754555 ]]\n",
      "\n",
      "  [[0.4754555 ]\n",
      "   [0.49218827]]\n",
      "\n",
      "  [[0.49218827]\n",
      "   [0.3373544 ]]\n",
      "\n",
      "  [[0.3373544 ]\n",
      "   [0.23728374]]\n",
      "\n",
      "  [[0.23728374]\n",
      "   [0.54841876]]\n",
      "\n",
      "  [[0.54841876]\n",
      "   [0.5902581 ]]\n",
      "\n",
      "  [[0.5902581 ]\n",
      "   [0.81925577]]\n",
      "\n",
      "  [[0.81925577]\n",
      "   [0.6879426 ]]\n",
      "\n",
      "  [[0.6879426 ]\n",
      "   [0.814187  ]]\n",
      "\n",
      "  [[0.814187  ]\n",
      "   [0.7518503 ]]\n",
      "\n",
      "  [[0.7518503 ]\n",
      "   [0.5827587 ]]\n",
      "\n",
      "  [[0.5827587 ]\n",
      "   [0.6663486 ]]\n",
      "\n",
      "  [[0.6663486 ]\n",
      "   [0.60965866]]\n",
      "\n",
      "  [[0.60965866]\n",
      "   [0.8690688 ]]\n",
      "\n",
      "  [[0.8690688 ]\n",
      "   [0.8627403 ]]\n",
      "\n",
      "  [[0.8627403 ]\n",
      "   [0.9969731 ]]\n",
      "\n",
      "  [[0.9969731 ]\n",
      "   [1.0560195 ]]\n",
      "\n",
      "  [[1.0560195 ]\n",
      "   [1.0142394 ]]\n",
      "\n",
      "  [[1.0142394 ]\n",
      "   [1.1393722 ]]\n",
      "\n",
      "  [[1.1393722 ]\n",
      "   [1.0464453 ]]\n",
      "\n",
      "  [[1.0464453 ]\n",
      "   [1.0704403 ]]\n",
      "\n",
      "  [[1.0704403 ]\n",
      "   [1.1816118 ]]]\n",
      "\n",
      "\n",
      " [[[1.1816118 ]\n",
      "   [1.1054324 ]]\n",
      "\n",
      "  [[1.1054324 ]\n",
      "   [0.97708344]]\n",
      "\n",
      "  [[0.97708344]\n",
      "   [1.0700846 ]]\n",
      "\n",
      "  [[1.0700846 ]\n",
      "   [1.0678465 ]]\n",
      "\n",
      "  [[1.0678465 ]\n",
      "   [1.1255592 ]]\n",
      "\n",
      "  [[1.1255592 ]\n",
      "   [1.1874216 ]]\n",
      "\n",
      "  [[1.1874216 ]\n",
      "   [1.1650568 ]]\n",
      "\n",
      "  [[1.1650568 ]\n",
      "   [1.2778735 ]]\n",
      "\n",
      "  [[1.2778735 ]\n",
      "   [1.2377385 ]]\n",
      "\n",
      "  [[1.2377385 ]\n",
      "   [1.1166222 ]]\n",
      "\n",
      "  [[1.1166222 ]\n",
      "   [1.1344517 ]]\n",
      "\n",
      "  [[1.1344517 ]\n",
      "   [1.1725414 ]]\n",
      "\n",
      "  [[1.1725414 ]\n",
      "   [1.14287   ]]\n",
      "\n",
      "  [[1.14287   ]\n",
      "   [1.1914381 ]]\n",
      "\n",
      "  [[1.1914381 ]\n",
      "   [1.2634823 ]]\n",
      "\n",
      "  [[1.2634823 ]\n",
      "   [1.2640604 ]]\n",
      "\n",
      "  [[1.2640604 ]\n",
      "   [1.1750313 ]]\n",
      "\n",
      "  [[1.1750313 ]\n",
      "   [1.100749  ]]\n",
      "\n",
      "  [[1.100749  ]\n",
      "   [1.1489168 ]]\n",
      "\n",
      "  [[1.1489168 ]\n",
      "   [1.1655163 ]]\n",
      "\n",
      "  [[1.1655163 ]\n",
      "   [1.2992156 ]]\n",
      "\n",
      "  [[1.2992156 ]\n",
      "   [1.2533152 ]]\n",
      "\n",
      "  [[1.2533152 ]\n",
      "   [1.3254485 ]]\n",
      "\n",
      "  [[1.3254485 ]\n",
      "   [1.2912123 ]]\n",
      "\n",
      "  [[1.2912123 ]\n",
      "   [1.3014979 ]]\n",
      "\n",
      "  [[1.3014979 ]\n",
      "   [1.3553275 ]]\n",
      "\n",
      "  [[1.3553275 ]\n",
      "   [1.421073  ]]\n",
      "\n",
      "  [[1.421073  ]\n",
      "   [1.4115876 ]]\n",
      "\n",
      "  [[1.4115876 ]\n",
      "   [1.4331964 ]]\n",
      "\n",
      "  [[1.4331964 ]\n",
      "   [1.4501219 ]]\n",
      "\n",
      "  [[1.4501219 ]\n",
      "   [1.4873075 ]]\n",
      "\n",
      "  [[1.4873075 ]\n",
      "   [1.5496293 ]]]\n",
      "\n",
      "\n",
      " [[[1.5496293 ]\n",
      "   [1.5340378 ]]\n",
      "\n",
      "  [[1.5340378 ]\n",
      "   [1.6549467 ]]\n",
      "\n",
      "  [[1.6549467 ]\n",
      "   [1.7119478 ]]\n",
      "\n",
      "  [[1.7119478 ]\n",
      "   [1.6745844 ]]\n",
      "\n",
      "  [[1.6745844 ]\n",
      "   [1.6493295 ]]\n",
      "\n",
      "  [[1.6493295 ]\n",
      "   [1.3706374 ]]\n",
      "\n",
      "  [[1.3706374 ]\n",
      "   [1.4287502 ]]\n",
      "\n",
      "  [[1.4287502 ]\n",
      "   [1.4662174 ]]\n",
      "\n",
      "  [[1.4662174 ]\n",
      "   [1.5524008 ]]\n",
      "\n",
      "  [[1.5524008 ]\n",
      "   [1.5357274 ]]\n",
      "\n",
      "  [[1.5357274 ]\n",
      "   [1.5384692 ]]\n",
      "\n",
      "  [[1.5384692 ]\n",
      "   [1.5123844 ]]\n",
      "\n",
      "  [[1.5123844 ]\n",
      "   [1.5422041 ]]\n",
      "\n",
      "  [[1.5422041 ]\n",
      "   [1.5621085 ]]\n",
      "\n",
      "  [[1.5621085 ]\n",
      "   [1.4421186 ]]\n",
      "\n",
      "  [[1.4421186 ]\n",
      "   [1.4916649 ]]\n",
      "\n",
      "  [[1.4916649 ]\n",
      "   [1.3809379 ]]\n",
      "\n",
      "  [[1.3809379 ]\n",
      "   [1.4464315 ]]\n",
      "\n",
      "  [[1.4464315 ]\n",
      "   [1.5161638 ]]\n",
      "\n",
      "  [[1.5161638 ]\n",
      "   [1.5392399 ]]\n",
      "\n",
      "  [[1.5392399 ]\n",
      "   [1.5602115 ]]\n",
      "\n",
      "  [[1.5602115 ]\n",
      "   [1.6338861 ]]\n",
      "\n",
      "  [[1.6338861 ]\n",
      "   [1.5829023 ]]\n",
      "\n",
      "  [[1.5829023 ]\n",
      "   [1.6193913 ]]\n",
      "\n",
      "  [[1.6193913 ]\n",
      "   [1.5928768 ]]\n",
      "\n",
      "  [[1.5928768 ]\n",
      "   [1.6417708 ]]\n",
      "\n",
      "  [[1.6417708 ]\n",
      "   [1.597575  ]]\n",
      "\n",
      "  [[1.597575  ]\n",
      "   [1.6602674 ]]\n",
      "\n",
      "  [[1.6602674 ]\n",
      "   [1.7033073 ]]\n",
      "\n",
      "  [[1.7033073 ]\n",
      "   [1.6870191 ]]\n",
      "\n",
      "  [[1.6870191 ]\n",
      "   [1.700595  ]]\n",
      "\n",
      "  [[1.700595  ]\n",
      "   [1.7407744 ]]]\n",
      "\n",
      "\n",
      " [[[1.7407744 ]\n",
      "   [1.7488667 ]]\n",
      "\n",
      "  [[1.7488667 ]\n",
      "   [1.7766113 ]]\n",
      "\n",
      "  [[1.7766113 ]\n",
      "   [1.7167943 ]]\n",
      "\n",
      "  [[1.7167943 ]\n",
      "   [1.687108  ]]\n",
      "\n",
      "  [[1.687108  ]\n",
      "   [1.722352  ]]\n",
      "\n",
      "  [[1.722352  ]\n",
      "   [1.6912726 ]]\n",
      "\n",
      "  [[1.6912726 ]\n",
      "   [1.7505562 ]]\n",
      "\n",
      "  [[1.7505562 ]\n",
      "   [1.7324451 ]]\n",
      "\n",
      "  [[1.7324451 ]\n",
      "   [1.7693491 ]]\n",
      "\n",
      "  [[1.7693491 ]\n",
      "   [1.8041635 ]]\n",
      "\n",
      "  [[1.8041635 ]\n",
      "   [1.8218002 ]]\n",
      "\n",
      "  [[1.8218002 ]\n",
      "   [1.8533095 ]]\n",
      "\n",
      "  [[1.8533095 ]\n",
      "   [1.8850113 ]]\n",
      "\n",
      "  [[1.8850113 ]\n",
      "   [1.8881534 ]]\n",
      "\n",
      "  [[1.8881534 ]\n",
      "   [1.9017738 ]]\n",
      "\n",
      "  [[1.9017738 ]\n",
      "   [1.8620834 ]]\n",
      "\n",
      "  [[1.8620834 ]\n",
      "   [1.9312377 ]]\n",
      "\n",
      "  [[1.9312377 ]\n",
      "   [1.9209816 ]]\n",
      "\n",
      "  [[1.9209816 ]\n",
      "   [1.920122  ]]\n",
      "\n",
      "  [[1.920122  ]\n",
      "   [1.9336684 ]]\n",
      "\n",
      "  [[1.9336684 ]\n",
      "   [1.9452138 ]]\n",
      "\n",
      "  [[1.9452138 ]\n",
      "   [1.9230863 ]]\n",
      "\n",
      "  [[1.9230863 ]\n",
      "   [1.9388853 ]]\n",
      "\n",
      "  [[1.9388853 ]\n",
      "   [1.9561516 ]]\n",
      "\n",
      "  [[1.9561516 ]\n",
      "   [2.0067205 ]]\n",
      "\n",
      "  [[2.0067205 ]\n",
      "   [2.0250096 ]]\n",
      "\n",
      "  [[2.0250096 ]\n",
      "   [2.0770457 ]]\n",
      "\n",
      "  [[2.0770457 ]\n",
      "   [2.0856714 ]]\n",
      "\n",
      "  [[2.0856714 ]\n",
      "   [2.1204412 ]]\n",
      "\n",
      "  [[2.1204412 ]\n",
      "   [2.1090293 ]]\n",
      "\n",
      "  [[2.1090293 ]\n",
      "   [2.1480675 ]]\n",
      "\n",
      "  [[2.1480675 ]\n",
      "   [2.2283819 ]]]\n",
      "\n",
      "\n",
      " [[[2.2283819 ]\n",
      "   [2.0419645 ]]\n",
      "\n",
      "  [[2.0419645 ]\n",
      "   [2.0003178 ]]\n",
      "\n",
      "  [[2.0003178 ]\n",
      "   [1.8593416 ]]\n",
      "\n",
      "  [[1.8593416 ]\n",
      "   [1.9588194 ]]\n",
      "\n",
      "  [[1.9588194 ]\n",
      "   [1.870235  ]]\n",
      "\n",
      "  [[1.870235  ]\n",
      "   [1.8728731 ]]\n",
      "\n",
      "  [[1.8728731 ]\n",
      "   [1.9359657 ]]\n",
      "\n",
      "  [[1.9359657 ]\n",
      "   [1.9621392 ]]\n",
      "\n",
      "  [[1.9621392 ]\n",
      "   [1.9388556 ]]\n",
      "\n",
      "  [[1.9388556 ]\n",
      "   [1.8966458 ]]\n",
      "\n",
      "  [[1.8966458 ]\n",
      "   [1.8410082 ]]\n",
      "\n",
      "  [[1.8410082 ]\n",
      "   [1.7840811 ]]\n",
      "\n",
      "  [[1.7840811 ]\n",
      "   [1.835228  ]]\n",
      "\n",
      "  [[1.835228  ]\n",
      "   [1.7186617 ]]\n",
      "\n",
      "  [[1.7186617 ]\n",
      "   [1.7329935 ]]\n",
      "\n",
      "  [[1.7329935 ]\n",
      "   [1.8098694 ]]\n",
      "\n",
      "  [[1.8098694 ]\n",
      "   [1.8886276 ]]\n",
      "\n",
      "  [[1.8886276 ]\n",
      "   [1.8647215 ]]\n",
      "\n",
      "  [[1.8647215 ]\n",
      "   [1.9055235 ]]\n",
      "\n",
      "  [[1.9055235 ]\n",
      "   [1.9319047 ]]\n",
      "\n",
      "  [[1.9319047 ]\n",
      "   [1.8839146 ]]\n",
      "\n",
      "  [[1.8839146 ]\n",
      "   [1.9731067 ]]\n",
      "\n",
      "  [[1.9731067 ]\n",
      "   [1.9025148 ]]\n",
      "\n",
      "  [[1.9025148 ]\n",
      "   [1.9891726 ]]\n",
      "\n",
      "  [[1.9891726 ]\n",
      "   [2.029767  ]]\n",
      "\n",
      "  [[2.029767  ]\n",
      "   [2.0746891 ]]\n",
      "\n",
      "  [[2.0746891 ]\n",
      "   [2.1592867 ]]\n",
      "\n",
      "  [[2.1592867 ]\n",
      "   [2.126251  ]]\n",
      "\n",
      "  [[2.126251  ]\n",
      "   [2.0917776 ]]\n",
      "\n",
      "  [[2.0917776 ]\n",
      "   [2.083878  ]]\n",
      "\n",
      "  [[2.083878  ]\n",
      "   [2.0845747 ]]\n",
      "\n",
      "  [[2.0845747 ]\n",
      "   [2.0002587 ]]]\n",
      "\n",
      "\n",
      " [[[2.0002587 ]\n",
      "   [2.0242684 ]]\n",
      "\n",
      "  [[2.0242684 ]\n",
      "   [2.013064  ]]\n",
      "\n",
      "  [[2.013064  ]\n",
      "   [2.0396378 ]]\n",
      "\n",
      "  [[2.0396378 ]\n",
      "   [2.0572746 ]]\n",
      "\n",
      "  [[2.0572746 ]\n",
      "   [1.9617984 ]]\n",
      "\n",
      "  [[1.9617984 ]\n",
      "   [1.9465477 ]]\n",
      "\n",
      "  [[1.9465477 ]\n",
      "   [1.7692157 ]]\n",
      "\n",
      "  [[1.7692157 ]\n",
      "   [1.8271358 ]]\n",
      "\n",
      "  [[1.8271358 ]\n",
      "   [1.7676299 ]]\n",
      "\n",
      "  [[1.7676299 ]\n",
      "   [1.8273284 ]]\n",
      "\n",
      "  [[1.8273284 ]\n",
      "   [1.9146532 ]]\n",
      "\n",
      "  [[1.9146532 ]\n",
      "   [2.0247428 ]]\n",
      "\n",
      "  [[2.0247428 ]\n",
      "   [2.1240575 ]]\n",
      "\n",
      "  [[2.1240575 ]\n",
      "   [2.1225607 ]]\n",
      "\n",
      "  [[2.1225607 ]\n",
      "   [2.1834152 ]]\n",
      "\n",
      "  [[2.1834152 ]\n",
      "   [2.1760492 ]]\n",
      "\n",
      "  [[2.1760492 ]\n",
      "   [2.2162583 ]]\n",
      "\n",
      "  [[2.2162583 ]\n",
      "   [2.1634219 ]]\n",
      "\n",
      "  [[2.1634219 ]\n",
      "   [2.2347696 ]]\n",
      "\n",
      "  [[2.2347696 ]\n",
      "   [2.2966616 ]]\n",
      "\n",
      "  [[2.2966616 ]\n",
      "   [2.2709029 ]]\n",
      "\n",
      "  [[2.2709029 ]\n",
      "   [2.2090406 ]]\n",
      "\n",
      "  [[2.2090406 ]\n",
      "   [2.2299085 ]]\n",
      "\n",
      "  [[2.2299085 ]\n",
      "   [2.193849  ]]\n",
      "\n",
      "  [[2.193849  ]\n",
      "   [2.223565  ]]\n",
      "\n",
      "  [[2.223565  ]\n",
      "   [2.3092594 ]]\n",
      "\n",
      "  [[2.3092594 ]\n",
      "   [2.3007226 ]]\n",
      "\n",
      "  [[2.3007226 ]\n",
      "   [2.3136168 ]]\n",
      "\n",
      "  [[2.3136168 ]\n",
      "   [2.2888362 ]]\n",
      "\n",
      "  [[2.2888362 ]\n",
      "   [2.3493352 ]]\n",
      "\n",
      "  [[2.3493352 ]\n",
      "   [2.3590577 ]]\n",
      "\n",
      "  [[2.3590577 ]\n",
      "   [2.3556638 ]]]]\n",
      "[[[[1.4534302 ]\n",
      "   [1.4403026 ]]\n",
      "\n",
      "  [[1.4688065 ]\n",
      "   [1.431728  ]]\n",
      "\n",
      "  [[1.4584434 ]\n",
      "   [1.4376816 ]]\n",
      "\n",
      "  [[1.4472178 ]\n",
      "   [1.4379058 ]]\n",
      "\n",
      "  [[1.4400992 ]\n",
      "   [1.4393077 ]]\n",
      "\n",
      "  [[1.453889  ]\n",
      "   [1.44734   ]]\n",
      "\n",
      "  [[1.4631087 ]\n",
      "   [1.4481411 ]]\n",
      "\n",
      "  [[1.4647162 ]\n",
      "   [1.4567935 ]]\n",
      "\n",
      "  [[1.4637306 ]\n",
      "   [1.4651095 ]]\n",
      "\n",
      "  [[1.4642607 ]\n",
      "   [1.45821   ]]\n",
      "\n",
      "  [[1.4759985 ]\n",
      "   [1.4517761 ]]\n",
      "\n",
      "  [[1.4961948 ]\n",
      "   [1.4394827 ]]\n",
      "\n",
      "  [[1.482714  ]\n",
      "   [1.4533165 ]]\n",
      "\n",
      "  [[1.5033906 ]\n",
      "   [1.4616624 ]]\n",
      "\n",
      "  [[1.5065632 ]\n",
      "   [1.4765962 ]]\n",
      "\n",
      "  [[1.5103655 ]\n",
      "   [1.4762745 ]]\n",
      "\n",
      "  [[1.5239077 ]\n",
      "   [1.4676144 ]]\n",
      "\n",
      "  [[1.5170319 ]\n",
      "   [1.478765  ]]\n",
      "\n",
      "  [[1.527573  ]\n",
      "   [1.4941957 ]]\n",
      "\n",
      "  [[1.5324492 ]\n",
      "   [1.5007961 ]]\n",
      "\n",
      "  [[1.5460618 ]\n",
      "   [1.4941233 ]]\n",
      "\n",
      "  [[1.5267525 ]\n",
      "   [1.5007944 ]]\n",
      "\n",
      "  [[1.5274055 ]\n",
      "   [1.4871159 ]]\n",
      "\n",
      "  [[1.5529795 ]\n",
      "   [1.4977798 ]]\n",
      "\n",
      "  [[1.5649425 ]\n",
      "   [1.4997845 ]]\n",
      "\n",
      "  [[1.5374649 ]\n",
      "   [1.5039492 ]]\n",
      "\n",
      "  [[1.553129  ]\n",
      "   [1.5065143 ]]\n",
      "\n",
      "  [[1.5553209 ]\n",
      "   [1.5179185 ]]\n",
      "\n",
      "  [[1.5629823 ]\n",
      "   [1.5248749 ]]\n",
      "\n",
      "  [[1.5774726 ]\n",
      "   [1.5190932 ]]\n",
      "\n",
      "  [[1.558095  ]\n",
      "   [1.5247128 ]]\n",
      "\n",
      "  [[1.5866842 ]\n",
      "   [1.5293899 ]]]\n",
      "\n",
      "\n",
      " [[[1.5843956 ]\n",
      "   [1.5299616 ]]\n",
      "\n",
      "  [[1.5922408 ]\n",
      "   [1.5340039 ]]\n",
      "\n",
      "  [[1.6099677 ]\n",
      "   [1.5302689 ]]\n",
      "\n",
      "  [[1.6073531 ]\n",
      "   [1.5422276 ]]\n",
      "\n",
      "  [[1.6097556 ]\n",
      "   [1.5504708 ]]\n",
      "\n",
      "  [[1.6105233 ]\n",
      "   [1.5535347 ]]\n",
      "\n",
      "  [[1.6167512 ]\n",
      "   [1.5474982 ]]\n",
      "\n",
      "  [[1.584179  ]\n",
      "   [1.549664  ]]\n",
      "\n",
      "  [[1.5628664 ]\n",
      "   [1.547806  ]]\n",
      "\n",
      "  [[1.583092  ]\n",
      "   [1.5525655 ]]\n",
      "\n",
      "  [[1.5821805 ]\n",
      "   [1.5565448 ]]\n",
      "\n",
      "  [[1.5888741 ]\n",
      "   [1.5457342 ]]\n",
      "\n",
      "  [[1.5566559 ]\n",
      "   [1.5450898 ]]\n",
      "\n",
      "  [[1.5667634 ]\n",
      "   [1.5427783 ]]\n",
      "\n",
      "  [[1.5910764 ]\n",
      "   [1.5402586 ]]\n",
      "\n",
      "  [[1.616074  ]\n",
      "   [1.539945  ]]\n",
      "\n",
      "  [[1.6299602 ]\n",
      "   [1.547533  ]]\n",
      "\n",
      "  [[1.6063374 ]\n",
      "   [1.5535612 ]]\n",
      "\n",
      "  [[1.6223927 ]\n",
      "   [1.5601518 ]]\n",
      "\n",
      "  [[1.6328403 ]\n",
      "   [1.5544883 ]]\n",
      "\n",
      "  [[1.6537814 ]\n",
      "   [1.5699353 ]]\n",
      "\n",
      "  [[1.659899  ]\n",
      "   [1.5808358 ]]\n",
      "\n",
      "  [[1.6527975 ]\n",
      "   [1.5919319 ]]\n",
      "\n",
      "  [[1.6534169 ]\n",
      "   [1.5953469 ]]\n",
      "\n",
      "  [[1.6610919 ]\n",
      "   [1.5972393 ]]\n",
      "\n",
      "  [[1.6605855 ]\n",
      "   [1.5922058 ]]\n",
      "\n",
      "  [[1.626318  ]\n",
      "   [1.594883  ]]\n",
      "\n",
      "  [[1.5734296 ]\n",
      "   [1.5896468 ]]\n",
      "\n",
      "  [[1.5157273 ]\n",
      "   [1.5756704 ]]\n",
      "\n",
      "  [[1.5032272 ]\n",
      "   [1.5655806 ]]\n",
      "\n",
      "  [[1.4116155 ]\n",
      "   [1.5402042 ]]\n",
      "\n",
      "  [[1.3799736 ]\n",
      "   [1.5248058 ]]]\n",
      "\n",
      "\n",
      " [[[1.4586551 ]\n",
      "   [1.5123769 ]]\n",
      "\n",
      "  [[1.3832151 ]\n",
      "   [1.4887528 ]]\n",
      "\n",
      "  [[1.4579546 ]\n",
      "   [1.4414732 ]]\n",
      "\n",
      "  [[1.3793976 ]\n",
      "   [1.4019016 ]]\n",
      "\n",
      "  [[1.3191292 ]\n",
      "   [1.3796355 ]]\n",
      "\n",
      "  [[1.1381861 ]\n",
      "   [1.3159701 ]]\n",
      "\n",
      "  [[1.2500823 ]\n",
      "   [1.2969141 ]]\n",
      "\n",
      "  [[1.1448858 ]\n",
      "   [1.3352486 ]]\n",
      "\n",
      "  [[0.8796793 ]\n",
      "   [1.2571648 ]]\n",
      "\n",
      "  [[1.0579726 ]\n",
      "   [1.2720666 ]]\n",
      "\n",
      "  [[0.6913835 ]\n",
      "   [1.179825  ]]\n",
      "\n",
      "  [[0.82082367]\n",
      "   [1.0975984 ]]\n",
      "\n",
      "  [[0.6494426 ]\n",
      "   [0.9536153 ]]\n",
      "\n",
      "  [[0.6224689 ]\n",
      "   [0.9672142 ]]\n",
      "\n",
      "  [[0.47371224]\n",
      "   [0.8156886 ]]\n",
      "\n",
      "  [[0.36488363]\n",
      "   [0.6142256 ]]\n",
      "\n",
      "  [[0.5806757 ]\n",
      "   [0.7331084 ]]\n",
      "\n",
      "  [[0.5848856 ]\n",
      "   [0.5149501 ]]\n",
      "\n",
      "  [[0.74643993]\n",
      "   [0.6130598 ]]\n",
      "\n",
      "  [[0.63410664]\n",
      "   [0.53304136]]\n",
      "\n",
      "  [[0.72575116]\n",
      "   [0.5472345 ]]\n",
      "\n",
      "  [[0.71385896]\n",
      "   [0.5060592 ]]\n",
      "\n",
      "  [[0.6182008 ]\n",
      "   [0.49961904]]\n",
      "\n",
      "  [[0.70742315]\n",
      "   [0.65674806]]\n",
      "\n",
      "  [[0.6644325 ]\n",
      "   [0.69376487]]\n",
      "\n",
      "  [[0.85578096]\n",
      "   [0.80389833]]\n",
      "\n",
      "  [[0.85157025]\n",
      "   [0.77289885]]\n",
      "\n",
      "  [[0.93770015]\n",
      "   [0.8326651 ]]\n",
      "\n",
      "  [[0.9682555 ]\n",
      "   [0.8188327 ]]\n",
      "\n",
      "  [[0.9561453 ]\n",
      "   [0.77500224]]\n",
      "\n",
      "  [[1.0594559 ]\n",
      "   [0.8457249 ]]\n",
      "\n",
      "  [[1.0249523 ]\n",
      "   [0.86546946]]]\n",
      "\n",
      "\n",
      " [[[1.0610425 ]\n",
      "   [0.9752679 ]]\n",
      "\n",
      "  [[1.1177917 ]\n",
      "   [1.0004104 ]]\n",
      "\n",
      "  [[1.0928345 ]\n",
      "   [1.067429  ]]\n",
      "\n",
      "  [[1.0254632 ]\n",
      "   [1.0822437 ]]\n",
      "\n",
      "  [[1.0784502 ]\n",
      "   [1.0714562 ]]\n",
      "\n",
      "  [[1.0821989 ]\n",
      "   [1.1007621 ]]\n",
      "\n",
      "  [[1.0986162 ]\n",
      "   [1.0752883 ]]\n",
      "\n",
      "  [[1.1367545 ]\n",
      "   [1.0984288 ]]\n",
      "\n",
      "  [[1.1229473 ]\n",
      "   [1.1365707 ]]\n",
      "\n",
      "  [[1.1857065 ]\n",
      "   [1.1139305 ]]\n",
      "\n",
      "  [[1.1836622 ]\n",
      "   [1.0811327 ]]\n",
      "\n",
      "  [[1.1147912 ]\n",
      "   [1.120692  ]]\n",
      "\n",
      "  [[1.1355665 ]\n",
      "   [1.1270025 ]]\n",
      "\n",
      "  [[1.1631293 ]\n",
      "   [1.1588929 ]]\n",
      "\n",
      "  [[1.1605957 ]\n",
      "   [1.187667  ]]\n",
      "\n",
      "  [[1.1915283 ]\n",
      "   [1.1791849 ]]\n",
      "\n",
      "  [[1.2117136 ]\n",
      "   [1.2208235 ]]\n",
      "\n",
      "  [[1.2223833 ]\n",
      "   [1.2056105 ]]\n",
      "\n",
      "  [[1.1778388 ]\n",
      "   [1.173617  ]]\n",
      "\n",
      "  [[1.141162  ]\n",
      "   [1.176454  ]]\n",
      "\n",
      "  [[1.1732423 ]\n",
      "   [1.1740761 ]]\n",
      "\n",
      "  [[1.1693676 ]\n",
      "   [1.1733451 ]]\n",
      "\n",
      "  [[1.2477624 ]\n",
      "   [1.2014768 ]]\n",
      "\n",
      "  [[1.2261101 ]\n",
      "   [1.2323841 ]]\n",
      "\n",
      "  [[1.2610937 ]\n",
      "   [1.2289375 ]]\n",
      "\n",
      "  [[1.2473586 ]\n",
      "   [1.1978912 ]]\n",
      "\n",
      "  [[1.2430444 ]\n",
      "   [1.1856321 ]]\n",
      "\n",
      "  [[1.2839844 ]\n",
      "   [1.2148403 ]]\n",
      "\n",
      "  [[1.3215637 ]\n",
      "   [1.2420647 ]]\n",
      "\n",
      "  [[1.3306313 ]\n",
      "   [1.2776089 ]]\n",
      "\n",
      "  [[1.3379095 ]\n",
      "   [1.2786382 ]]\n",
      "\n",
      "  [[1.3569537 ]\n",
      "   [1.3096963 ]]]\n",
      "\n",
      "\n",
      " [[[1.3845336 ]\n",
      "   [1.3147823 ]]\n",
      "\n",
      "  [[1.42132   ]\n",
      "   [1.3375008 ]]\n",
      "\n",
      "  [[1.4270942 ]\n",
      "   [1.3573639 ]]\n",
      "\n",
      "  [[1.4683781 ]\n",
      "   [1.388523  ]]\n",
      "\n",
      "  [[1.5016882 ]\n",
      "   [1.4098927 ]]\n",
      "\n",
      "  [[1.5005841 ]\n",
      "   [1.425823  ]]\n",
      "\n",
      "  [[1.5011787 ]\n",
      "   [1.4385965 ]]\n",
      "\n",
      "  [[1.391015  ]\n",
      "   [1.4306744 ]]\n",
      "\n",
      "  [[1.402697  ]\n",
      "   [1.433291  ]]\n",
      "\n",
      "  [[1.4266634 ]\n",
      "   [1.4390241 ]]\n",
      "\n",
      "  [[1.4659358 ]\n",
      "   [1.4668609 ]]\n",
      "\n",
      "  [[1.4621506 ]\n",
      "   [1.4744034 ]]\n",
      "\n",
      "  [[1.4494812 ]\n",
      "   [1.4472203 ]]\n",
      "\n",
      "  [[1.4121834 ]\n",
      "   [1.4274486 ]]\n",
      "\n",
      "  [[1.4254217 ]\n",
      "   [1.3727899 ]]\n",
      "\n",
      "  [[1.442023  ]\n",
      "   [1.3977473 ]]\n",
      "\n",
      "  [[1.3973243 ]\n",
      "   [1.4084258 ]]\n",
      "\n",
      "  [[1.4217179 ]\n",
      "   [1.4084814 ]]\n",
      "\n",
      "  [[1.3503652 ]\n",
      "   [1.4034979 ]]\n",
      "\n",
      "  [[1.3831072 ]\n",
      "   [1.3999676 ]]\n",
      "\n",
      "  [[1.4302715 ]\n",
      "   [1.4042598 ]]\n",
      "\n",
      "  [[1.4422004 ]\n",
      "   [1.4083691 ]]\n",
      "\n",
      "  [[1.4368311 ]\n",
      "   [1.4139423 ]]\n",
      "\n",
      "  [[1.4712883 ]\n",
      "   [1.3988543 ]]\n",
      "\n",
      "  [[1.4643133 ]\n",
      "   [1.4210659 ]]\n",
      "\n",
      "  [[1.4806046 ]\n",
      "   [1.4109979 ]]\n",
      "\n",
      "  [[1.4770275 ]\n",
      "   [1.4255402 ]]\n",
      "\n",
      "  [[1.4914893 ]\n",
      "   [1.4442639 ]]\n",
      "\n",
      "  [[1.4865394 ]\n",
      "   [1.4573245 ]]\n",
      "\n",
      "  [[1.5139811 ]\n",
      "   [1.4691303 ]]\n",
      "\n",
      "  [[1.5372624 ]\n",
      "   [1.4874824 ]]\n",
      "\n",
      "  [[1.5348654 ]\n",
      "   [1.4729927 ]]]\n",
      "\n",
      "\n",
      " [[[1.5243096 ]\n",
      "   [1.483684  ]]\n",
      "\n",
      "  [[1.5451916 ]\n",
      "   [1.4868046 ]]\n",
      "\n",
      "  [[1.5531759 ]\n",
      "   [1.503074  ]]\n",
      "\n",
      "  [[1.5684758 ]\n",
      "   [1.5027288 ]]\n",
      "\n",
      "  [[1.554379  ]\n",
      "   [1.5062877 ]]\n",
      "\n",
      "  [[1.5250746 ]\n",
      "   [1.5121237 ]]\n",
      "\n",
      "  [[1.542592  ]\n",
      "   [1.510889  ]]\n",
      "\n",
      "  [[1.5341196 ]\n",
      "   [1.518486  ]]\n",
      "\n",
      "  [[1.5606074 ]\n",
      "   [1.5253195 ]]\n",
      "\n",
      "  [[1.5583984 ]\n",
      "   [1.5235445 ]]\n",
      "\n",
      "  [[1.5698123 ]\n",
      "   [1.5289233 ]]\n",
      "\n",
      "  [[1.584712  ]\n",
      "   [1.5289782 ]]\n",
      "\n",
      "  [[1.5928088 ]\n",
      "   [1.5349395 ]]\n",
      "\n",
      "  [[1.6123618 ]\n",
      "   [1.5476224 ]]\n",
      "\n",
      "  [[1.6298991 ]\n",
      "   [1.546136  ]]\n",
      "\n",
      "  [[1.6222918 ]\n",
      "   [1.566587  ]]\n",
      "\n",
      "  [[1.6328689 ]\n",
      "   [1.5732064 ]]\n",
      "\n",
      "  [[1.6298611 ]\n",
      "   [1.5811985 ]]\n",
      "\n",
      "  [[1.6608568 ]\n",
      "   [1.591624  ]]\n",
      "\n",
      "  [[1.6652567 ]\n",
      "   [1.5918196 ]]\n",
      "\n",
      "  [[1.6509209 ]\n",
      "   [1.6002333 ]]\n",
      "\n",
      "  [[1.6613026 ]\n",
      "   [1.6062897 ]]\n",
      "\n",
      "  [[1.6656724 ]\n",
      "   [1.6121593 ]]\n",
      "\n",
      "  [[1.6639556 ]\n",
      "   [1.6139303 ]]\n",
      "\n",
      "  [[1.6718378 ]\n",
      "   [1.6019373 ]]\n",
      "\n",
      "  [[1.661707  ]\n",
      "   [1.6141734 ]]\n",
      "\n",
      "  [[1.6824254 ]\n",
      "   [1.6168137 ]]\n",
      "\n",
      "  [[1.6899636 ]\n",
      "   [1.6244742 ]]\n",
      "\n",
      "  [[1.7091279 ]\n",
      "   [1.6308177 ]]\n",
      "\n",
      "  [[1.7150086 ]\n",
      "   [1.6271666 ]]\n",
      "\n",
      "  [[1.7092313 ]\n",
      "   [1.6367536 ]]\n",
      "\n",
      "  [[1.7334478 ]\n",
      "   [1.6459746 ]]]\n",
      "\n",
      "\n",
      " [[[1.7455572 ]\n",
      "   [1.6631219 ]]\n",
      "\n",
      "  [[1.7747486 ]\n",
      "   [1.6784856 ]]\n",
      "\n",
      "  [[1.735958  ]\n",
      "   [1.680419  ]]\n",
      "\n",
      "  [[1.7141185 ]\n",
      "   [1.6816375 ]]\n",
      "\n",
      "  [[1.6752856 ]\n",
      "   [1.6807971 ]]\n",
      "\n",
      "  [[1.7013732 ]\n",
      "   [1.6831117 ]]\n",
      "\n",
      "  [[1.6881429 ]\n",
      "   [1.6677858 ]]\n",
      "\n",
      "  [[1.6725626 ]\n",
      "   [1.6690907 ]]\n",
      "\n",
      "  [[1.6879026 ]\n",
      "   [1.6625069 ]]\n",
      "\n",
      "  [[1.6897297 ]\n",
      "   [1.6415066 ]]\n",
      "\n",
      "  [[1.6796825 ]\n",
      "   [1.6339784 ]]\n",
      "\n",
      "  [[1.6645538 ]\n",
      "   [1.6061232 ]]\n",
      "\n",
      "  [[1.6284796 ]\n",
      "   [1.6141315 ]]\n",
      "\n",
      "  [[1.6114384 ]\n",
      "   [1.5992233 ]]\n",
      "\n",
      "  [[1.6292338 ]\n",
      "   [1.6019672 ]]\n",
      "\n",
      "  [[1.5953126 ]\n",
      "   [1.6062878 ]]\n",
      "\n",
      "  [[1.5973661 ]\n",
      "   [1.5885632 ]]\n",
      "\n",
      "  [[1.5977556 ]\n",
      "   [1.5825603 ]]\n",
      "\n",
      "  [[1.6251873 ]\n",
      "   [1.5786643 ]]\n",
      "\n",
      "  [[1.6172311 ]\n",
      "   [1.5778742 ]]\n",
      "\n",
      "  [[1.6473287 ]\n",
      "   [1.574181  ]]\n",
      "\n",
      "  [[1.6558007 ]\n",
      "   [1.5807983 ]]\n",
      "\n",
      "  [[1.6282864 ]\n",
      "   [1.5733551 ]]\n",
      "\n",
      "  [[1.668124  ]\n",
      "   [1.5858185 ]]\n",
      "\n",
      "  [[1.6563334 ]\n",
      "   [1.6103684 ]]\n",
      "\n",
      "  [[1.6886315 ]\n",
      "   [1.6235008 ]]\n",
      "\n",
      "  [[1.7046994 ]\n",
      "   [1.6237419 ]]\n",
      "\n",
      "  [[1.7106959 ]\n",
      "   [1.6340064 ]]\n",
      "\n",
      "  [[1.7440078 ]\n",
      "   [1.6469469 ]]\n",
      "\n",
      "  [[1.7387013 ]\n",
      "   [1.6547215 ]]\n",
      "\n",
      "  [[1.7357538 ]\n",
      "   [1.6649779 ]]\n",
      "\n",
      "  [[1.7367628 ]\n",
      "   [1.6530609 ]]]\n",
      "\n",
      "\n",
      " [[[1.7274172 ]\n",
      "   [1.6680157 ]]\n",
      "\n",
      "  [[1.7126812 ]\n",
      "   [1.6725256 ]]\n",
      "\n",
      "  [[1.7207891 ]\n",
      "   [1.6777103 ]]\n",
      "\n",
      "  [[1.7222252 ]\n",
      "   [1.682618  ]]\n",
      "\n",
      "  [[1.7285697 ]\n",
      "   [1.6668738 ]]\n",
      "\n",
      "  [[1.7133461 ]\n",
      "   [1.6649189 ]]\n",
      "\n",
      "  [[1.6879046 ]\n",
      "   [1.6600405 ]]\n",
      "\n",
      "  [[1.6778827 ]\n",
      "   [1.6541501 ]]\n",
      "\n",
      "  [[1.6209714 ]\n",
      "   [1.637382  ]]\n",
      "\n",
      "  [[1.6397904 ]\n",
      "   [1.6246529 ]]\n",
      "\n",
      "  [[1.6207587 ]\n",
      "   [1.6252981 ]]\n",
      "\n",
      "  [[1.6396291 ]\n",
      "   [1.6273706 ]]\n",
      "\n",
      "  [[1.6631196 ]\n",
      "   [1.6299789 ]]\n",
      "\n",
      "  [[1.695959  ]\n",
      "   [1.6246154 ]]\n",
      "\n",
      "  [[1.7272363 ]\n",
      "   [1.6221569 ]]\n",
      "\n",
      "  [[1.7133515 ]\n",
      "   [1.6178555 ]]\n",
      "\n",
      "  [[1.7373799 ]\n",
      "   [1.6383177 ]]\n",
      "\n",
      "  [[1.7458372 ]\n",
      "   [1.6460848 ]]\n",
      "\n",
      "  [[1.7687231 ]\n",
      "   [1.666613  ]]\n",
      "\n",
      "  [[1.769218  ]\n",
      "   [1.6767585 ]]\n",
      "\n",
      "  [[1.7789968 ]\n",
      "   [1.6950619 ]]\n",
      "\n",
      "  [[1.8041232 ]\n",
      "   [1.7116814 ]]\n",
      "\n",
      "  [[1.8026377 ]\n",
      "   [1.7207613 ]]\n",
      "\n",
      "  [[1.7935607 ]\n",
      "   [1.7242994 ]]\n",
      "\n",
      "  [[1.7992839 ]\n",
      "   [1.715315  ]]\n",
      "\n",
      "  [[1.7756541 ]\n",
      "   [1.7201074 ]]\n",
      "\n",
      "  [[1.7879887 ]\n",
      "   [1.7181158 ]]\n",
      "\n",
      "  [[1.8121428 ]\n",
      "   [1.7322543 ]]\n",
      "\n",
      "  [[1.8169355 ]\n",
      "   [1.7277228 ]]\n",
      "\n",
      "  [[1.7995474 ]\n",
      "   [1.7302439 ]]\n",
      "\n",
      "  [[1.8158652 ]\n",
      "   [1.7269913 ]]\n",
      "\n",
      "  [[1.8299057 ]\n",
      "   [1.7393098 ]]]]\n",
      "test shape: (8, 32, 2, 1) (8, 32, 2, 1)\n",
      "test shape: (256, 2, 1) (256, 2, 1)\n",
      "mse:0.07607422769069672, mae:0.23271825909614563\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    \n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = 'data' # root path of data file\n",
    "args.data_path = 'sp500_nrm.csv' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'Close' # target feature in S or MS task\n",
    "args.freq = 'd' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = 'informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 72 # input sequence length of Informer encoder\n",
    "args.label_len = 16 # start token length of Informer decoder\n",
    "args.pred_len = 48 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 5 # encoder input size\n",
    "args.dec_in = 5 # decoder input size\n",
    "args.c_out = 5 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 10 # num of heads\n",
    "args.e_layers = 4 # num of encoder layers\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'd'\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 20\n",
    "args.patience = 5\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = True\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "VUcvSLlkSFTx"
   },
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'custom':{'data':'sp500_nrm.csv','T':'Close','M':[5,5,5],'S':[1,1,1],'MS':[5,5,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom', 'root_path': 'data', 'data_path': 'sp500_nrm.csv', 'features': 'MS', 'target': 'Close', 'freq': 'd', 'checkpoints': 'informer_checkpoints', 'seq_len': 72, 'label_len': 16, 'pred_len': 48, 'enc_in': 5, 'dec_in': 5, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 10, 'e_layers': 4, 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 20, 'patience': 5, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': True, 'devices': '0,1,2,3', 'device_ids': [0, 1, 2, 3], 'detail_freq': 'd'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'sp500_nrm_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : sp500_nrm_informer_custom_ftMS_sl72_ll16_pl48_dm512_nh10_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "data.data_loader.Dataset_Custom\n",
      "train\n",
      "data_path ->  sp500_nrm.csv\n",
      "scaling\n",
      "data.data_loader.Dataset_Custom\n",
      "val\n",
      "data_path ->  sp500_nrm.csv\n",
      "scaling\n",
      "data.data_loader.Dataset_Custom\n",
      "test\n",
      "data_path ->  sp500_nrm.csv\n",
      "scaling\n",
      "loss ->  tensor(0.8338, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.9607, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.5505, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.4041, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.5191, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3444, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3331, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3815, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.4321, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.4120, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2215, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2356, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3107, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3631, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2710, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2097, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.3292, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.4301, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2683, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2196, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2371, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2369, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2966, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2163, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1850, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2856, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2099, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1979, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2280, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1999, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2173, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1740, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2083, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2173, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2005, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2074, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2286, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1976, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1931, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1914, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1852, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1812, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.2419, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1692, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1997, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1699, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1655, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1908, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1584, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1463, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1604, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1637, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1313, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1309, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1442, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1441, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1422, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1619, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1888, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1297, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1685, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1717, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1419, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 1 cost time: 7.650643825531006\n",
      "Epoch: 1, Steps: 65 | Train Loss: 0.2539628 Vali Loss: 0.3483150 Test Loss: 0.7683111\n",
      "Validation loss decreased (inf --> 0.348315).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "loss ->  tensor(0.1659, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1772, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1465, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1671, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1607, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1320, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1665, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1499, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1397, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1914, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1759, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1167, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1479, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1374, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1334, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1350, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1195, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1198, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1189, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1393, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1229, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1133, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1149, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1198, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1110, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1031, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1266, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1154, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1226, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1297, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1236, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1038, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1071, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1265, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1127, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1161, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1109, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1095, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1073, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1080, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1122, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1156, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1075, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1050, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1062, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1121, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1099, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1107, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1165, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1131, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1103, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1275, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1332, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1124, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1067, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1095, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1170, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1163, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1113, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1002, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1034, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1083, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0952, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1164, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1478, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 2 cost time: 7.6302783489227295\n",
      "Epoch: 2, Steps: 65 | Train Loss: 0.1241519 Vali Loss: 0.1788954 Test Loss: 0.5562099\n",
      "Validation loss decreased (0.348315 --> 0.178895).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "loss ->  tensor(0.1173, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0981, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1230, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1132, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1145, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0968, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1174, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1223, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1141, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0993, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1122, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1117, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1076, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0921, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1130, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1061, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1103, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1063, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0997, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1061, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0997, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0991, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0980, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0877, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0934, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1057, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1007, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0950, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1092, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1065, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1016, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0962, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0938, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1008, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0959, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0974, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1024, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1055, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1030, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1033, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1019, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0919, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1007, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1053, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1073, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0882, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1009, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1124, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0959, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0915, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1033, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1086, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0915, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1008, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0963, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0942, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1081, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0940, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0968, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1157, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1026, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1121, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1062, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1132, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1068, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 3 cost time: 7.645060062408447\n",
      "Epoch: 3, Steps: 65 | Train Loss: 0.1034158 Vali Loss: 0.1872396 Test Loss: 0.5645803\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "loss ->  tensor(0.1069, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1233, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0963, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0811, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0982, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0900, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0883, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1006, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1032, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0936, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0946, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0948, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1014, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0863, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1030, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0840, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0925, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0940, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0945, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0962, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0896, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0940, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0801, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0854, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1062, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0969, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0891, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0769, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0867, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0910, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0820, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0950, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1016, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0893, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0866, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0958, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0834, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0800, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0887, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0901, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0813, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0876, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0771, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0795, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0922, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0898, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0866, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0969, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0807, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0865, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0815, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0890, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0879, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0876, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0892, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0875, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0864, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0984, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1116, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0990, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0920, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 4 cost time: 7.618279933929443\n",
      "Epoch: 4, Steps: 65 | Train Loss: 0.0908999 Vali Loss: 0.1558555 Test Loss: 0.5076247\n",
      "Validation loss decreased (0.178895 --> 0.155855).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "loss ->  tensor(0.0839, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0988, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0969, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0914, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0787, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0879, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0939, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0755, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1079, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0823, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0965, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0960, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1015, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0825, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0910, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0808, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0937, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0864, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0974, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0857, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0981, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0791, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0864, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0873, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0850, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0832, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0878, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0749, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0863, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0744, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0924, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0931, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0799, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0850, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0861, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0802, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0853, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0919, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0874, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0957, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0758, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0917, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0854, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0877, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0855, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0863, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0849, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0812, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0785, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0836, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0920, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0744, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0912, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0822, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0944, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0789, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0904, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0928, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0967, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0878, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 5 cost time: 7.6962339878082275\n",
      "Epoch: 5, Steps: 65 | Train Loss: 0.0870005 Vali Loss: 0.1794589 Test Loss: 0.5516404\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "loss ->  tensor(0.0943, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0800, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0985, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0838, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0772, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0792, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0709, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0931, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0798, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0974, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0842, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0800, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0894, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0811, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0821, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0853, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0824, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0862, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0861, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0810, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0765, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0793, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0880, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0871, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0868, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0852, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0847, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0820, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0876, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0846, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0816, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0892, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0886, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0845, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0714, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0836, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0854, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0783, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0777, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0763, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0856, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0844, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0740, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0746, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0822, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0921, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0742, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0876, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0852, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0787, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0833, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0972, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0847, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0795, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0834, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0933, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0828, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0952, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0767, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 6 cost time: 7.633647680282593\n",
      "Epoch: 6, Steps: 65 | Train Loss: 0.0835040 Vali Loss: 0.1848499 Test Loss: 0.5471879\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "loss ->  tensor(0.0736, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0891, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0809, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0817, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0868, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0713, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0739, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0874, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0823, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0745, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0719, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0769, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0946, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0835, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0692, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0898, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0857, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0823, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0863, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0782, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0768, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0737, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0846, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0847, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0851, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0827, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0748, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0804, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0816, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0739, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0891, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0762, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0913, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0852, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0739, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0797, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0876, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0756, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0971, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0906, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0933, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0886, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0809, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0874, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0749, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0837, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0750, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0803, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0798, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0735, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0808, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0835, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0835, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0787, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0783, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0782, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0926, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0849, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0874, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0763, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 7 cost time: 7.4979939460754395\n",
      "Epoch: 7, Steps: 65 | Train Loss: 0.0817016 Vali Loss: 0.1673135 Test Loss: 0.5255975\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "loss ->  tensor(0.0785, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0700, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0750, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0805, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0860, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0808, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0787, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0826, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0852, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0745, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0727, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0712, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0897, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0831, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0917, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0931, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0821, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0716, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0726, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0757, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0834, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0845, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0819, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0873, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0825, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0733, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0859, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0709, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0857, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0715, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0776, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0790, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0730, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0835, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0841, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0837, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0913, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0765, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0726, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0847, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0870, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0835, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0888, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0872, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0807, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0803, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0927, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0921, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0869, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0919, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0772, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0889, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0814, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0803, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0886, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0803, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0830, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0793, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0811, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0788, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0928, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 8 cost time: 7.4807798862457275\n",
      "Epoch: 8, Steps: 65 | Train Loss: 0.0816742 Vali Loss: 0.1801229 Test Loss: 0.5439726\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "loss ->  tensor(0.0729, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0696, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0807, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0754, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0768, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0821, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0841, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0873, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0777, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0817, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0732, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0699, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0757, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0752, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0759, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0760, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0878, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0740, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0778, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0854, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0945, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0782, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0751, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0774, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0901, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0786, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0728, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0954, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0928, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0868, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0969, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0816, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0716, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0770, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0808, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0880, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0869, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0754, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0794, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0800, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0773, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0862, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0799, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0797, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0796, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0758, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.1026, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0780, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0804, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0990, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0812, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0709, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0832, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0780, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0884, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0741, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0709, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0853, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0837, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0834, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "loss ->  tensor(0.0779, device='cuda:0', grad_fn=<L1LossBackward0>)\n",
      "Epoch: 9 cost time: 7.474092960357666\n",
      "Epoch: 9, Steps: 65 | Train Loss: 0.0808014 Vali Loss: 0.1718528 Test Loss: 0.5315430\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : sp500_nrm_informer_custom_ftMS_sl72_ll16_pl48_dm512_nh10_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "data.data_loader.Dataset_Custom\n",
      "test\n",
      "data_path ->  sp500_nrm.csv\n",
      "scaling\n",
      "[[[[1.5952184 ]\n",
      "   [1.57647   ]\n",
      "   [1.5362905 ]\n",
      "   ...\n",
      "   [1.80858   ]\n",
      "   [1.8635656 ]\n",
      "   [1.8800019 ]]\n",
      "\n",
      "  [[1.57647   ]\n",
      "   [1.5362905 ]\n",
      "   [1.5056558 ]\n",
      "   ...\n",
      "   [1.8635656 ]\n",
      "   [1.8800019 ]\n",
      "   [1.8532206 ]]\n",
      "\n",
      "  [[1.5362905 ]\n",
      "   [1.5056558 ]\n",
      "   [1.5346454 ]\n",
      "   ...\n",
      "   [1.8800019 ]\n",
      "   [1.8532206 ]\n",
      "   [1.8893539 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.7607975 ]\n",
      "   [1.7945595 ]\n",
      "   [1.7871786 ]\n",
      "   ...\n",
      "   [0.6697426 ]\n",
      "   [0.4754555 ]\n",
      "   [0.49218827]]\n",
      "\n",
      "  [[1.7945595 ]\n",
      "   [1.7871786 ]\n",
      "   [1.7962787 ]\n",
      "   ...\n",
      "   [0.4754555 ]\n",
      "   [0.49218827]\n",
      "   [0.3373544 ]]\n",
      "\n",
      "  [[1.7871786 ]\n",
      "   [1.7962787 ]\n",
      "   [1.8370658 ]\n",
      "   ...\n",
      "   [0.49218827]\n",
      "   [0.3373544 ]\n",
      "   [0.23728374]]]\n",
      "\n",
      "\n",
      " [[[1.7962787 ]\n",
      "   [1.8370658 ]\n",
      "   [1.8560513 ]\n",
      "   ...\n",
      "   [0.3373544 ]\n",
      "   [0.23728374]\n",
      "   [0.54841876]]\n",
      "\n",
      "  [[1.8370658 ]\n",
      "   [1.8560513 ]\n",
      "   [1.8429645 ]\n",
      "   ...\n",
      "   [0.23728374]\n",
      "   [0.54841876]\n",
      "   [0.5902581 ]]\n",
      "\n",
      "  [[1.8560513 ]\n",
      "   [1.8429645 ]\n",
      "   [1.8443873 ]\n",
      "   ...\n",
      "   [0.54841876]\n",
      "   [0.5902581 ]\n",
      "   [0.81925577]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.3360455 ]\n",
      "   [1.299675  ]\n",
      "   [1.501254  ]\n",
      "   ...\n",
      "   [1.1166222 ]\n",
      "   [1.1344517 ]\n",
      "   [1.1725414 ]]\n",
      "\n",
      "  [[1.299675  ]\n",
      "   [1.501254  ]\n",
      "   [1.3725197 ]\n",
      "   ...\n",
      "   [1.1344517 ]\n",
      "   [1.1725414 ]\n",
      "   [1.14287   ]]\n",
      "\n",
      "  [[1.501254  ]\n",
      "   [1.3725197 ]\n",
      "   [1.5603745 ]\n",
      "   ...\n",
      "   [1.1725414 ]\n",
      "   [1.14287   ]\n",
      "   [1.1914381 ]]]\n",
      "\n",
      "\n",
      " [[[1.3725197 ]\n",
      "   [1.5603745 ]\n",
      "   [1.4030063 ]\n",
      "   ...\n",
      "   [1.14287   ]\n",
      "   [1.1914381 ]\n",
      "   [1.2634823 ]]\n",
      "\n",
      "  [[1.5603745 ]\n",
      "   [1.4030063 ]\n",
      "   [1.3265749 ]\n",
      "   ...\n",
      "   [1.1914381 ]\n",
      "   [1.2634823 ]\n",
      "   [1.2640604 ]]\n",
      "\n",
      "  [[1.4030063 ]\n",
      "   [1.3265749 ]\n",
      "   [0.9919043 ]\n",
      "   ...\n",
      "   [1.2634823 ]\n",
      "   [1.2640604 ]\n",
      "   [1.1750313 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.1393722 ]\n",
      "   [1.0464453 ]\n",
      "   [1.0704403 ]\n",
      "   ...\n",
      "   [1.5357274 ]\n",
      "   [1.5384692 ]\n",
      "   [1.5123844 ]]\n",
      "\n",
      "  [[1.0464453 ]\n",
      "   [1.0704403 ]\n",
      "   [1.1816118 ]\n",
      "   ...\n",
      "   [1.5384692 ]\n",
      "   [1.5123844 ]\n",
      "   [1.5422041 ]]\n",
      "\n",
      "  [[1.0704403 ]\n",
      "   [1.1816118 ]\n",
      "   [1.1054324 ]\n",
      "   ...\n",
      "   [1.5123844 ]\n",
      "   [1.5422041 ]\n",
      "   [1.5621085 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.5496293 ]\n",
      "   [1.5340378 ]\n",
      "   [1.6549467 ]\n",
      "   ...\n",
      "   [1.8850113 ]\n",
      "   [1.8881534 ]\n",
      "   [1.9017738 ]]\n",
      "\n",
      "  [[1.5340378 ]\n",
      "   [1.6549467 ]\n",
      "   [1.7119478 ]\n",
      "   ...\n",
      "   [1.8881534 ]\n",
      "   [1.9017738 ]\n",
      "   [1.8620834 ]]\n",
      "\n",
      "  [[1.6549467 ]\n",
      "   [1.7119478 ]\n",
      "   [1.6745844 ]\n",
      "   ...\n",
      "   [1.9017738 ]\n",
      "   [1.8620834 ]\n",
      "   [1.9312377 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.7033073 ]\n",
      "   [1.6870191 ]\n",
      "   [1.700595  ]\n",
      "   ...\n",
      "   [1.8966458 ]\n",
      "   [1.8410082 ]\n",
      "   [1.7840811 ]]\n",
      "\n",
      "  [[1.6870191 ]\n",
      "   [1.700595  ]\n",
      "   [1.7407744 ]\n",
      "   ...\n",
      "   [1.8410082 ]\n",
      "   [1.7840811 ]\n",
      "   [1.835228  ]]\n",
      "\n",
      "  [[1.700595  ]\n",
      "   [1.7407744 ]\n",
      "   [1.7488667 ]\n",
      "   ...\n",
      "   [1.7840811 ]\n",
      "   [1.835228  ]\n",
      "   [1.7186617 ]]]\n",
      "\n",
      "\n",
      " [[[1.7407744 ]\n",
      "   [1.7488667 ]\n",
      "   [1.7766113 ]\n",
      "   ...\n",
      "   [1.835228  ]\n",
      "   [1.7186617 ]\n",
      "   [1.7329935 ]]\n",
      "\n",
      "  [[1.7488667 ]\n",
      "   [1.7766113 ]\n",
      "   [1.7167943 ]\n",
      "   ...\n",
      "   [1.7186617 ]\n",
      "   [1.7329935 ]\n",
      "   [1.8098694 ]]\n",
      "\n",
      "  [[1.7766113 ]\n",
      "   [1.7167943 ]\n",
      "   [1.687108  ]\n",
      "   ...\n",
      "   [1.7329935 ]\n",
      "   [1.8098694 ]\n",
      "   [1.8886276 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.1204412 ]\n",
      "   [2.1090293 ]\n",
      "   [2.1480675 ]\n",
      "   ...\n",
      "   [1.8273284 ]\n",
      "   [1.9146532 ]\n",
      "   [2.0247428 ]]\n",
      "\n",
      "  [[2.1090293 ]\n",
      "   [2.1480675 ]\n",
      "   [2.2283819 ]\n",
      "   ...\n",
      "   [1.9146532 ]\n",
      "   [2.0247428 ]\n",
      "   [2.1240575 ]]\n",
      "\n",
      "  [[2.1480675 ]\n",
      "   [2.2283819 ]\n",
      "   [2.0419645 ]\n",
      "   ...\n",
      "   [2.0247428 ]\n",
      "   [2.1240575 ]\n",
      "   [2.1225607 ]]]\n",
      "\n",
      "\n",
      " [[[2.2283819 ]\n",
      "   [2.0419645 ]\n",
      "   [2.0003178 ]\n",
      "   ...\n",
      "   [2.1240575 ]\n",
      "   [2.1225607 ]\n",
      "   [2.1834152 ]]\n",
      "\n",
      "  [[2.0419645 ]\n",
      "   [2.0003178 ]\n",
      "   [1.8593416 ]\n",
      "   ...\n",
      "   [2.1225607 ]\n",
      "   [2.1834152 ]\n",
      "   [2.1760492 ]]\n",
      "\n",
      "  [[2.0003178 ]\n",
      "   [1.8593416 ]\n",
      "   [1.9588194 ]\n",
      "   ...\n",
      "   [2.1834152 ]\n",
      "   [2.1760492 ]\n",
      "   [2.2162583 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.0917776 ]\n",
      "   [2.083878  ]\n",
      "   [2.0845747 ]\n",
      "   ...\n",
      "   [2.438305  ]\n",
      "   [2.418934  ]\n",
      "   [2.3974586 ]]\n",
      "\n",
      "  [[2.083878  ]\n",
      "   [2.0845747 ]\n",
      "   [2.0002587 ]\n",
      "   ...\n",
      "   [2.418934  ]\n",
      "   [2.3974586 ]\n",
      "   [2.3861058 ]]\n",
      "\n",
      "  [[2.0845747 ]\n",
      "   [2.0002587 ]\n",
      "   [2.0242684 ]\n",
      "   ...\n",
      "   [2.3974586 ]\n",
      "   [2.3861058 ]\n",
      "   [2.3901815 ]]]]\n",
      "[[[[1.2214578 ]\n",
      "   [1.2549384 ]\n",
      "   [1.2911345 ]\n",
      "   ...\n",
      "   [1.2433258 ]\n",
      "   [1.2210404 ]\n",
      "   [1.315692  ]]\n",
      "\n",
      "  [[1.2249001 ]\n",
      "   [1.226879  ]\n",
      "   [1.2592165 ]\n",
      "   ...\n",
      "   [1.3143197 ]\n",
      "   [1.2282609 ]\n",
      "   [1.2405409 ]]\n",
      "\n",
      "  [[1.2316629 ]\n",
      "   [1.2204287 ]\n",
      "   [1.2724521 ]\n",
      "   ...\n",
      "   [1.2690763 ]\n",
      "   [1.260169  ]\n",
      "   [1.2523723 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.1001225 ]\n",
      "   [1.2050384 ]\n",
      "   [1.2890981 ]\n",
      "   ...\n",
      "   [1.2281654 ]\n",
      "   [1.1982857 ]\n",
      "   [1.2875361 ]]\n",
      "\n",
      "  [[1.2760949 ]\n",
      "   [1.2042987 ]\n",
      "   [1.3366187 ]\n",
      "   ...\n",
      "   [1.2374094 ]\n",
      "   [1.2772084 ]\n",
      "   [1.2646532 ]]\n",
      "\n",
      "  [[1.2739855 ]\n",
      "   [1.2075225 ]\n",
      "   [1.3526034 ]\n",
      "   ...\n",
      "   [1.2257991 ]\n",
      "   [1.2801414 ]\n",
      "   [1.2917931 ]]]\n",
      "\n",
      "\n",
      " [[[1.1353064 ]\n",
      "   [1.2054741 ]\n",
      "   [1.3388795 ]\n",
      "   ...\n",
      "   [1.2677841 ]\n",
      "   [1.2486771 ]\n",
      "   [1.2959403 ]]\n",
      "\n",
      "  [[1.1368723 ]\n",
      "   [1.2013162 ]\n",
      "   [1.2835242 ]\n",
      "   ...\n",
      "   [1.272238  ]\n",
      "   [1.2991884 ]\n",
      "   [1.2398303 ]]\n",
      "\n",
      "  [[1.2210739 ]\n",
      "   [1.2082644 ]\n",
      "   [1.2886128 ]\n",
      "   ...\n",
      "   [1.3454196 ]\n",
      "   [1.3035022 ]\n",
      "   [1.2891415 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.1871629 ]\n",
      "   [1.2976862 ]\n",
      "   [1.021207  ]\n",
      "   ...\n",
      "   [1.2924607 ]\n",
      "   [1.1526845 ]\n",
      "   [1.3891696 ]]\n",
      "\n",
      "  [[1.1983513 ]\n",
      "   [1.3553684 ]\n",
      "   [1.0081292 ]\n",
      "   ...\n",
      "   [1.2614819 ]\n",
      "   [1.0706705 ]\n",
      "   [1.3288068 ]]\n",
      "\n",
      "  [[1.2804598 ]\n",
      "   [1.2893924 ]\n",
      "   [1.0717895 ]\n",
      "   ...\n",
      "   [1.2698146 ]\n",
      "   [1.1680759 ]\n",
      "   [1.3531784 ]]]\n",
      "\n",
      "\n",
      " [[[1.112017  ]\n",
      "   [1.3095107 ]\n",
      "   [1.1531527 ]\n",
      "   ...\n",
      "   [1.315512  ]\n",
      "   [1.1127744 ]\n",
      "   [1.2923912 ]]\n",
      "\n",
      "  [[1.295066  ]\n",
      "   [1.2913971 ]\n",
      "   [1.296906  ]\n",
      "   ...\n",
      "   [1.3058597 ]\n",
      "   [1.118717  ]\n",
      "   [1.2850775 ]]\n",
      "\n",
      "  [[1.2943088 ]\n",
      "   [1.2976303 ]\n",
      "   [1.301183  ]\n",
      "   ...\n",
      "   [1.3050262 ]\n",
      "   [1.1182026 ]\n",
      "   [1.2440134 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9214889 ]\n",
      "   [0.90749764]\n",
      "   [0.7800207 ]\n",
      "   ...\n",
      "   [1.0329896 ]\n",
      "   [0.9866032 ]\n",
      "   [1.0475332 ]]\n",
      "\n",
      "  [[0.9398265 ]\n",
      "   [0.90164506]\n",
      "   [0.90474397]\n",
      "   ...\n",
      "   [1.0356301 ]\n",
      "   [1.0536567 ]\n",
      "   [1.1119229 ]]\n",
      "\n",
      "  [[0.9610938 ]\n",
      "   [0.90180135]\n",
      "   [0.909592  ]\n",
      "   ...\n",
      "   [1.0803571 ]\n",
      "   [1.0660497 ]\n",
      "   [1.0773286 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.1155801 ]\n",
      "   [1.183124  ]\n",
      "   [1.1202258 ]\n",
      "   ...\n",
      "   [1.228279  ]\n",
      "   [1.110577  ]\n",
      "   [1.2108481 ]]\n",
      "\n",
      "  [[1.1209685 ]\n",
      "   [1.2070036 ]\n",
      "   [1.1573788 ]\n",
      "   ...\n",
      "   [1.2074004 ]\n",
      "   [1.1702836 ]\n",
      "   [1.1761656 ]]\n",
      "\n",
      "  [[1.1200958 ]\n",
      "   [1.1364324 ]\n",
      "   [1.1332165 ]\n",
      "   ...\n",
      "   [1.1799393 ]\n",
      "   [1.1565648 ]\n",
      "   [1.1251587 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.2329242 ]\n",
      "   [1.3353893 ]\n",
      "   [1.1826489 ]\n",
      "   ...\n",
      "   [1.3211138 ]\n",
      "   [1.1842579 ]\n",
      "   [1.1978753 ]]\n",
      "\n",
      "  [[1.1550919 ]\n",
      "   [1.3545897 ]\n",
      "   [1.2163515 ]\n",
      "   ...\n",
      "   [1.3316383 ]\n",
      "   [1.228657  ]\n",
      "   [1.1674508 ]]\n",
      "\n",
      "  [[1.242606  ]\n",
      "   [1.3521092 ]\n",
      "   [1.2341014 ]\n",
      "   ...\n",
      "   [1.3412758 ]\n",
      "   [1.2260867 ]\n",
      "   [1.2959545 ]]]\n",
      "\n",
      "\n",
      " [[[1.2526407 ]\n",
      "   [1.3133458 ]\n",
      "   [1.1516439 ]\n",
      "   ...\n",
      "   [1.2806559 ]\n",
      "   [1.0933671 ]\n",
      "   [1.2516586 ]]\n",
      "\n",
      "  [[1.2589587 ]\n",
      "   [1.2017963 ]\n",
      "   [1.258738  ]\n",
      "   ...\n",
      "   [1.3565024 ]\n",
      "   [1.0847836 ]\n",
      "   [1.2563103 ]]\n",
      "\n",
      "  [[1.270554  ]\n",
      "   [1.2133394 ]\n",
      "   [1.2562034 ]\n",
      "   ...\n",
      "   [1.3560784 ]\n",
      "   [0.9891639 ]\n",
      "   [1.240715  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.3291814 ]\n",
      "   [1.2139558 ]\n",
      "   [1.2483526 ]\n",
      "   ...\n",
      "   [1.326935  ]\n",
      "   [1.3353144 ]\n",
      "   [1.3841394 ]]\n",
      "\n",
      "  [[1.3413346 ]\n",
      "   [1.2266933 ]\n",
      "   [1.3425646 ]\n",
      "   ...\n",
      "   [1.3356693 ]\n",
      "   [1.2740679 ]\n",
      "   [1.3798126 ]]\n",
      "\n",
      "  [[1.2295264 ]\n",
      "   [1.2152158 ]\n",
      "   [1.3618584 ]\n",
      "   ...\n",
      "   [1.3448212 ]\n",
      "   [1.2225772 ]\n",
      "   [1.3404516 ]]]\n",
      "\n",
      "\n",
      " [[[1.3358449 ]\n",
      "   [1.3037114 ]\n",
      "   [1.3642402 ]\n",
      "   ...\n",
      "   [1.3864119 ]\n",
      "   [1.2901708 ]\n",
      "   [1.3306437 ]]\n",
      "\n",
      "  [[1.3268716 ]\n",
      "   [1.3150492 ]\n",
      "   [1.3603296 ]\n",
      "   ...\n",
      "   [1.4117471 ]\n",
      "   [1.2523373 ]\n",
      "   [1.3302774 ]]\n",
      "\n",
      "  [[1.3306388 ]\n",
      "   [1.4367248 ]\n",
      "   [1.3553414 ]\n",
      "   ...\n",
      "   [1.3642254 ]\n",
      "   [1.2462586 ]\n",
      "   [1.3670737 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.3163029 ]\n",
      "   [1.2479736 ]\n",
      "   [1.2046257 ]\n",
      "   ...\n",
      "   [1.3387374 ]\n",
      "   [1.2350192 ]\n",
      "   [1.3374606 ]]\n",
      "\n",
      "  [[1.3399419 ]\n",
      "   [1.2218316 ]\n",
      "   [1.2543126 ]\n",
      "   ...\n",
      "   [1.34955   ]\n",
      "   [1.3031353 ]\n",
      "   [1.2279309 ]]\n",
      "\n",
      "  [[1.2941309 ]\n",
      "   [1.3548533 ]\n",
      "   [1.1662786 ]\n",
      "   ...\n",
      "   [1.3541062 ]\n",
      "   [1.3048984 ]\n",
      "   [1.2559006 ]]]]\n",
      "test shape: (7, 32, 48, 1) (7, 32, 48, 1)\n",
      "test shape: (224, 48, 1) (224, 48, 1)\n",
      "mse:0.31497350335121155, mae:0.5064499378204346\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    \n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
